{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a251b6ea",
   "metadata": {},
   "source": [
    "# Mini Dataset Episode Sampler\n",
    "\n",
    "该脚本用于从 `demo_data_language` 数据集中按不同随机种子抽取小规模 episode 子集，便于快速实验 SmolVLA 在小批量数据下的训练表现。运行之前请确认原始数据集已完整存在。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96cfd7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"采样配置\"\"\"\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "# 每个子集包含的 episode 数量\n",
    "EPISODES_PER_SUBSET = 15\n",
    "\n",
    "# 使用的随机种子（每个种子采样一次）\n",
    "RANDOM_SEEDS = [0, 1, 2]\n",
    "\n",
    "# 数据路径配置\n",
    "SOURCE_ROOT = Path(\"demo_data_language\")\n",
    "TARGET_ROOT = Path(\"demo_data_language_subsets_nbr15\")\n",
    "\n",
    "# 若目标子集目录已存在，是否允许覆盖\n",
    "OVERWRITE_EXISTING = True\n",
    "\n",
    "# 是否重新编号 episode（默认保留原始编号以便对照）\n",
    "RENUMBER_EPISODES = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dbdbe6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subset': 'seed_000',\n",
       "  'episodes': [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 17, 19],\n",
       "  'total_frames': 3193},\n",
       " {'subset': 'seed_001',\n",
       "  'episodes': [0, 1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 15, 16, 18],\n",
       "  'total_frames': 3136},\n",
       " {'subset': 'seed_002',\n",
       "  'episodes': [0, 1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 18],\n",
       "  'total_frames': 3203}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"生成小规模 episode 子集\"\"\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    entries = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            entries.append(json.loads(line))\n",
    "    return entries\n",
    "\n",
    "\n",
    "def write_jsonl(path: Path, entries):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in entries:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def ensure_dir(path: Path):\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "SOURCE_DATA_DIR = SOURCE_ROOT / \"data\" / \"chunk-000\"\n",
    "SOURCE_IMAGE_DIRS = {\n",
    "    \"observation.image\": SOURCE_ROOT / \"images\" / \"observation.image\",\n",
    "    \"observation.wrist_image\": SOURCE_ROOT / \"images\" / \"observation.wrist_image\",\n",
    "}\n",
    "SOURCE_META_DIR = SOURCE_ROOT / \"meta\"\n",
    "BLOCK_POSE_LOG_PATH = SOURCE_ROOT / \"remove_red_block_from_plate_UR5_smolvla_mujoco\" / \"block_pose_log.json\"\n",
    "\n",
    "if not SOURCE_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"未找到原始数据集目录: {SOURCE_ROOT}\")\n",
    "\n",
    "all_episode_indices = sorted(int(p.stem.split(\"_\")[1]) for p in SOURCE_DATA_DIR.glob(\"episode_*.parquet\"))\n",
    "if len(all_episode_indices) < EPISODES_PER_SUBSET:\n",
    "    raise ValueError(\"原始数据集中的 episode 数量不足以完成采样，请检查 EPISODES_PER_SUBSET 配置。\")\n",
    "\n",
    "episodes_meta = load_jsonl(SOURCE_META_DIR / \"episodes.jsonl\")\n",
    "episodes_meta_map = {entry[\"episode_index\"]: entry for entry in episodes_meta}\n",
    "\n",
    "episodes_stats = load_jsonl(SOURCE_META_DIR / \"episodes_stats.jsonl\")\n",
    "episodes_stats_map = {entry[\"episode_index\"]: entry for entry in episodes_stats}\n",
    "\n",
    "block_pose_lookup = {}\n",
    "if BLOCK_POSE_LOG_PATH.exists():\n",
    "    raw_block_pose = json.loads(BLOCK_POSE_LOG_PATH.read_text(encoding=\"utf-8\"))\n",
    "    for item in raw_block_pose:\n",
    "        block_pose_lookup[int(item[\"episode\"])] = item\n",
    "\n",
    "info_template = json.loads((SOURCE_META_DIR / \"info.json\").read_text(encoding=\"utf-8\"))\n",
    "tasks_payload = (SOURCE_META_DIR / \"tasks.jsonl\").read_text(encoding=\"utf-8\")\n",
    "\n",
    "summary = []\n",
    "\n",
    "for seed in RANDOM_SEEDS:\n",
    "    random.seed(seed)\n",
    "    chosen = sorted(random.sample(all_episode_indices, EPISODES_PER_SUBSET))\n",
    "\n",
    "    subset_name = f\"seed_{seed:03d}\"\n",
    "    dest_root = TARGET_ROOT / subset_name\n",
    "\n",
    "    if dest_root.exists():\n",
    "        if not OVERWRITE_EXISTING:\n",
    "            raise FileExistsError(f\"目标目录已存在: {dest_root}，若需覆盖请将 OVERWRITE_EXISTING 设为 True\")\n",
    "        shutil.rmtree(dest_root)\n",
    "\n",
    "    dest_chunk_dir = dest_root / \"data\" / \"chunk-000\"\n",
    "    dest_obs_dir = dest_root / \"images\" / \"observation.image\"\n",
    "    dest_wrist_dir = dest_root / \"images\" / \"observation.wrist_image\"\n",
    "    dest_meta_dir = dest_root / \"meta\"\n",
    "\n",
    "    for path in [dest_chunk_dir, dest_obs_dir, dest_wrist_dir, dest_meta_dir]:\n",
    "        ensure_dir(path)\n",
    "\n",
    "    new_episodes_entries = []\n",
    "    new_stats_entries = []\n",
    "    new_block_pose_entries = []\n",
    "    total_frames = 0\n",
    "\n",
    "    global_frame_index = 0\n",
    "\n",
    "    for new_idx, orig_idx in enumerate(chosen):\n",
    "        target_idx = new_idx if RENUMBER_EPISODES else orig_idx\n",
    "\n",
    "        src_data_file = SOURCE_DATA_DIR / f\"episode_{orig_idx:06d}.parquet\"\n",
    "        dst_data_file = dest_chunk_dir / f\"episode_{target_idx:06d}.parquet\"\n",
    "        shutil.copy2(src_data_file, dst_data_file)\n",
    "\n",
    "        table = pq.read_table(dst_data_file)\n",
    "        num_rows = table.num_rows\n",
    "        if RENUMBER_EPISODES:\n",
    "            if \"episode_index\" in table.schema.names:\n",
    "                idx = table.schema.get_field_index(\"episode_index\")\n",
    "                table = table.set_column(idx, \"episode_index\", pa.array([target_idx] * num_rows, type=table.column(idx).type))\n",
    "            if \"index\" in table.schema.names:\n",
    "                idx = table.schema.get_field_index(\"index\")\n",
    "                start = global_frame_index\n",
    "                new_index = pa.array(range(start, start + num_rows), type=table.column(idx).type)\n",
    "                table = table.set_column(idx, \"index\", new_index)\n",
    "            global_frame_index += num_rows\n",
    "            pq.write_table(table, dst_data_file)\n",
    "        else:\n",
    "            global_frame_index += num_rows\n",
    "\n",
    "        for key, src_root in SOURCE_IMAGE_DIRS.items():\n",
    "            src_dir = src_root / f\"episode_{orig_idx:06d}\"\n",
    "            dst_dir = (dest_root / \"images\" / key) / f\"episode_{target_idx:06d}\"\n",
    "            shutil.copytree(src_dir, dst_dir)\n",
    "\n",
    "        episode_entry = deepcopy(episodes_meta_map[orig_idx])\n",
    "        total_frames += int(episode_entry.get(\"length\", num_rows))\n",
    "        episode_entry[\"episode_index\"] = target_idx\n",
    "        new_episodes_entries.append(episode_entry)\n",
    "\n",
    "        stats_entry = deepcopy(episodes_stats_map[orig_idx])\n",
    "        stats_entry[\"episode_index\"] = target_idx\n",
    "        stats_payload = stats_entry.get(\"stats\", {})\n",
    "        episode_stats = stats_payload.get(\"episode_index\")\n",
    "        if episode_stats:\n",
    "            count = episode_stats.get(\"count\", [num_rows])\n",
    "            stats_payload[\"episode_index\"] = {\n",
    "                \"min\": [target_idx],\n",
    "                \"max\": [target_idx],\n",
    "                \"mean\": [float(target_idx)],\n",
    "                \"std\": [0.0],\n",
    "                \"count\": count,\n",
    "            }\n",
    "        index_stats = stats_payload.get(\"index\")\n",
    "        if RENUMBER_EPISODES and index_stats:\n",
    "            start = global_frame_index - num_rows\n",
    "            end = start + num_rows - 1\n",
    "            mean = start + (num_rows - 1) / 2 if num_rows > 0 else start\n",
    "            std = sqrt((num_rows ** 2 - 1) / 12) if num_rows > 1 else 0.0\n",
    "            stats_payload[\"index\"] = {\n",
    "                \"min\": [start],\n",
    "                \"max\": [end],\n",
    "                \"mean\": [mean],\n",
    "                \"std\": [std],\n",
    "                \"count\": index_stats.get(\"count\", [num_rows]),\n",
    "            }\n",
    "        new_stats_entries.append(stats_entry)\n",
    "\n",
    "        pose_entry = block_pose_lookup.get(orig_idx)\n",
    "        if pose_entry:\n",
    "            pose_copy = deepcopy(pose_entry)\n",
    "            pose_copy[\"episode\"] = target_idx if RENUMBER_EPISODES else pose_entry[\"episode\"]\n",
    "            pose_copy[\"source_episode\"] = pose_entry[\"episode\"]\n",
    "            new_block_pose_entries.append(pose_copy)\n",
    "\n",
    "    write_jsonl(dest_meta_dir / \"episodes.jsonl\", new_episodes_entries)\n",
    "    write_jsonl(dest_meta_dir / \"episodes_stats.jsonl\", new_stats_entries)\n",
    "    (dest_meta_dir / \"tasks.jsonl\").write_text(tasks_payload, encoding=\"utf-8\")\n",
    "\n",
    "    info = deepcopy(info_template)\n",
    "    info[\"total_episodes\"] = EPISODES_PER_SUBSET\n",
    "    info[\"total_frames\"] = total_frames\n",
    "    info[\"splits\"] = {\"train\": f\"0:{EPISODES_PER_SUBSET}\"}\n",
    "    info[\"subset_seed\"] = seed\n",
    "    info[\"subset_source_episodes\"] = chosen\n",
    "    info[\"generated_at\"] = datetime.now().isoformat(timespec=\"seconds\")\n",
    "    info[\"renumbered\"] = RENUMBER_EPISODES\n",
    "    (dest_meta_dir / \"info.json\").write_text(json.dumps(info, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    if new_block_pose_entries:\n",
    "        (dest_root / \"block_pose_log.json\").write_text(json.dumps(new_block_pose_entries, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    summary.append({\"subset\": subset_name, \"episodes\": chosen, \"total_frames\": total_frames})\n",
    "\n",
    "summary\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_vla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
